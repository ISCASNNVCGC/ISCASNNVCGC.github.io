<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PSK098C3TV"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-PSK098C3TV');
    </script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="GrandChallenge">
    <meta name="description" content="GrandChallenge">
    <meta name="author" content="Sponsored by ByteDance">
    
    <title>
        
            Challenge |
        
        Grand Challenge
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/">
    
<link rel="stylesheet" href="/fontawesome/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/regular.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/brands.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"iscasnnvcgc.github.io","root":"/","language":"en"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true},"style":{"primary_color":"#1E90FF","img_position":"center","avatar":"/images/avatar.svg","font_size":null,"font_family":null,"hover":{"shadow":true,"scale":false},"first_screen":{"enable":true,"header_transparent":false,"background_img":"/images/polygon.svg","description":"Welcome to the Grand Challenge on Neural Network-based Video Coding (ISCAS 2024) || May 19-22, 2024, Singapore","font_color":"#FFFFFF","hitokoto":false},"scroll":{"progress_bar":false,"percent":false}},"local_search":{"enable":false,"preload":false},"code_copy":{},"code_block":{"tools":{"enable":true,"style":"default","code_copy":true},"highlight_theme":"default"},"side_tools":{},"pjax":{"enable":false},"lazyload":{"enable":false},"comment":{"enable":true,"use":"waline","valine":{"appid":null,"appkey":null,"placeholder":null},"gitalk":{"github_id":null,"github_admins":null,"repository":null,"client_id":null,"client_secret":null},"twikoo":{"env_id":null,"region":null,"version":"1.6.7"},"waline":{"server_url":"https://gccomment-edooxoymc-mtkawakarpo.vercel.app/","reaction":false,"version":2}},"post":{"author_label":{"enable":true,"auto":true,"custom_label_list":["Trainee","Engineer","Architect"]},"word_count":{"enable":false,"wordcount":false,"min2read":false},"img_align":"left","copyright_info":false},"version":"3.5.2"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    KEEP.language_code_block = {"copy":"Copy code","copied":"Copied","fold":"Fold code block","folded":"Folded"};
  </script>
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>


<body>
<div class="progress-bar-container">
    

    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/">
               Grand Challenge
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class="active"
                               href="/challenge"
                            >
                                CHALLENGE
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/participation"
                            >
                                PARTICIPATION
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/submission"
                            >
                                SUBMISSION
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/leaderboard"
                            >
                                LEADERBOARD
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/faq"
                            >
                                FAQ
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/organizer"
                            >
                                ORGANIZERS
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/history"
                            >
                                HISTORY
                            </a>
                        </li>
                    
                    
                </ul>
            </div>
            <div class="mobile">
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class="active"
                       href="/challenge">CHALLENGE</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/participation">PARTICIPATION</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/submission">SUBMISSION</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/leaderboard">LEADERBOARD</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/faq">FAQ</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/organizer">ORGANIZERS</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/history">HISTORY</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    
<div class="fade-in-down-animation">
    <div class="page-template-container">
        
        
        <div class="page-template-content keep-markdown-body">
            
                <h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a><b><span style="color:#1E90FF;">Abstract</span></b></h1><p>Recently, there is increasing interest in neural network-based video coding, including end-to-end and hybrid schemes. To foster the research in this emerging field and provide a benchmark, we propose this Grand Challenge (GC). In this GC, different neural network-based coding schemes will be evaluated according to their coding efficiency and innovations in methodologies. Three tracks will be evaluated, including: </p>
<ul>
<li>hybrid neural network-based (NN-based) video codec, </li>
<li>end-to-end video codec,</li>
<li>neural network enhanced VVC encoder.</li>
</ul>
<p>In the hybrid codec track, deep network-based coding tools shall be used with traditional video coding schemes. In the end-to-end codec track, the whole video codec system shall be built primarily upon deep networks. In the neural network enhanced VVC encoder track, deep network-based encoding algorithms can be applied in a VVC encoder which generates VVC compatible bitstreams.</p>
<p>Participants shall express their interest to participate in this Grand Challenge by sending an email to the organizer Dr. Yue Li and are invited to submit their schemes as ISCAS papers. The papers will be regularly reviewed and, if accepted, must be presented at ISCAS 2024. The submission instructions for Grand Challenge papers will be communicated by the organizers.</p>
<h1 id="Rationale"><a href="#Rationale" class="headerlink" title="Rationale"></a><b><span style="color:#1E90FF;">Rationale</span></b></h1><p>In recent years, deep learning-based image/video coding schemes have achieved remarkable progress. As two representative approaches aiming at future video codec schemes, hybrid solutions and end-to-end solutions have both been investigated extensively. Hybrid solutions adopt deep network-based coding tools to enhance traditional video coding schemes while end-to-end solutions build the whole compression scheme based on deep networks. Besides, NN-based methods are also widely studied to optimize or speed up encoders compliant to existing popular standards such as VVC. Although great advancement has been observed, there are still numerous challenges remaining to be addressed:</p>
<ul>
<li>How to harmonize a deep coding tool with a hybrid video codec, for example, how to take compression process into consideration when developing a deep tool for pre-processing;</li>
<li>How to exploit long-term temporal dependency in an end-to-end framework for video coding;</li>
<li>How to leverage automated machine learning-based network architecture optimization for higher coding efficiency;</li>
<li>How to perform efficient bit allocation with deep learning frameworks;</li>
<li>How to achieve a better global result in terms of rate-distortion trade-offs, for example, to take the impact of the current step on later frames into account, possibly by using reinforcement learning;</li>
<li>How to achieve better complexity-efficiency trade-offs;</li>
<li>How to speed up a VVC encoder with less coding efficiency loss via NN methods or use NN-based preprocessing to enhance the VVC encoding efficiency.</li>
</ul>
<p>In view of these challenges, several activities towards improving deep-learning-based image/video coding schemes have been initiated. For example, a special section on “Learning-based Image and Video Compression” was published in TCSVT, July 2020; a special section on “Optimized Image/Video Coding Based on Deep Learning” was published in OJCAS, December 2021; and the “Challenge on Learned Image Compression (CLIC)” at CVPR has been organized annually since 2018. In hopes of encouraging more innovative contributions towards the aforementioned challenges in the ISCAS community, we proposed this grand challenge since 2022. It has been successfully held for two years (ISCAS 2022, ISCAS 2023), attracting related researchers all over the world. As being looked forward by many experts in this area, the grand challenge will be held again for ISCAS 2024, with more tracks and more awards.</p>
<h1 id="Requirements-Evaluation"><a href="#Requirements-Evaluation" class="headerlink" title="Requirements, Evaluation"></a><b><span style="color:#1E90FF;">Requirements, Evaluation</span></b></h1><h3 id="Training-Data-Set"><a href="#Training-Data-Set" class="headerlink" title="Training Data Set"></a>Training Data Set</h3><p>It is recommended to use the following training data.<br>UVG dataset: <a class="link" target="_blank" rel="noopener" href="http://ultravideo.cs.tut.fi/">http://ultravideo.cs.tut.fi/<i class="fas fa-external-link-alt"></i></a><br>CDVL dataset: <a class="link" target="_blank" rel="noopener" href="https://cdvl.org/">https://cdvl.org/<i class="fas fa-external-link-alt"></i></a><br>Additional training data are also allowed to be used given that they are described in the submitted document.</p>
<h3 id="Test-Specifications"><a href="#Test-Specifications" class="headerlink" title="Test Specifications"></a>Test Specifications</h3><p>In the test, each scheme will be evaluated with multiple YUV 4:2:0 test sequences in the resolution of 1920x1080.<br>There is no constraint on the reference structure. Note that the neural network must be used in the decoding process of the hybrid track and the end-to-end track, while the VVC reference software VTM will be utilized for decoding bitstreams of the NN enhanced VVC encoder-only track.</p>
<h3 id="Evaluation-Criteria"><a href="#Evaluation-Criteria" class="headerlink" title="Evaluation Criteria"></a>Evaluation Criteria</h3><p>The test sequences will be released according to the timeline and the results will be evaluated with the following criteria:</p>
<ul>
<li>The decoded sequences will be evaluated in the 4:2:0 color format.</li>
<li>PSNR (6*PSNRY + PSNRU + PSNRV)/8 will be used to evaluate the distortion of the decoded pictures.</li>
</ul>
<p>Average Bjøntegaard delta bitrates (BDR) [1] for all test sequences will be gathered to compare the coding efficiency.<br>Anchors of HM 16.22 [2] and VTM-20.2 [3] coded with QPs = {22, 27, 32, 37} under the random access configurations defined in the HM and VTM common test conditions [4, 5] will be provided. Note that the HM anchor is used for the hybrid and end-to-end tracks, while the VTM anchor is used for the VVC encoder-only track. The released anchor data will include the bit-rates corresponding to the four QPs for each sequence.</p>
<p>Additional constraints for the first two tracks (i.e., the hybrid NN-based and end-to-end video codec) are listed as follows:</p>
<ul>
<li>It is required that the proposed method should generate four bit-streams for each sequence, targeting the anchor bit-rates corresponding to the four QPs. For each sequence, the range of four real bit-rates shall be [80% * the lowest anchor bit-rate, 120% * the highest anchor bit-rate];</li>
<li>Only one single decoder shall be utilized to decode all the bitstreams;</li>
<li>The intra period in the proposed submission shall be no larger than that used by the anchor in compressing the validation and test sequences.</li>
</ul>
<p>While for the NN enhanced VVC encoder track, the additional requirements are listed as follows:</p>
<ul>
<li>The docker file shall have the capability of encoding the test sequences to generate VTM-compatible bitstreams.</li>
<li><span style="color:#FF0000;">It is required that the proposed method should generate four bit-streams for each sequence, targeting at the anchor bit-rates corresponding to the four QPs. For each test point, the bit-rate of the proposed method should be in the range of 90% to 110% of the anchor bit-rate.</span></li>
<li>The VTM-20.2 decoder is utilized to decode generated bitstreams to get reconstructed YUV files and use those YUV files to calculate the PSNR values. All the generated bitstreams MUST be decoded successfully.</li>
<li>The VTM-20.2 encoder is utilized as the anchor encoder. For each test point, denote the encoding time of the proposed encoder as T1, the encoding time of VTM-20.2 encoder as T2, T1 and T2 should satisfy: T1 &lt;= 70% T2. Note that T1 and T2 shall be evaluated on the same platform with single thread (e.g., Intel(R) Xeon(R) Platinum 8336C CPU @ 2.30GHz, NVIDIA A100-SXM4-80GB GPU). Encoding time comparison will be verified by the organizers.</li>
</ul>
<!-- While for the NN enhanced VVC encoder track, the additional requirements are listed as follows:
1.	The docker file shall have the capability of encoding the test sequences to generate VTM-compatible bitstreams.
2.	The VTM-20.2 decoder is utilized to decode generated bitstreams to get reconstructed YUV files and use those YUV files to calculate the PSNR values. All the generated bitstreams MUST be decoded successfully.
3.	The VTM-20.2 encoder is utilized as the anchor encoder. For each test point, denote the encoding time of the proposed encoder as T1, the encoding time of VTM-20.2 encoder as T2, T1 and T2 should satisfy: T1 <= 70% T2. Note that T1 and T2 shall be evaluated on the same platform (e.g. Intel(R) Xeon(R) Platinum 8336C CPU @ 2.30GHz, NVIDIA A100-SXM4-80GB GPU). -->

<h3 id="Proposed-Documents"><a href="#Proposed-Documents" class="headerlink" title="Proposed Documents"></a>Proposed Documents</h3><p>A docker container with the executable scheme must be submitted for result generation and cross-check. Each participant is invited to submit an ISCAS paper, which must describe the following items in detail.</p>
<ul>
<li>The methodology</li>
<li>The training data set</li>
<li>Detailed rate-distortion data (comparison with the provided anchor is encouraged)</li>
<li>Complexity analysis of the proposed solutions is encouraged for the paper submission.</li>
</ul>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1] Bjøntegaard, “Calculation of average PSNR differences between RD-Curves,” ITUT SG16/Q6, Doc. VCEG-M33, Austin, Apr. 2001.<br>[2] <a class="link" target="_blank" rel="noopener" href="https://vcgit.hhi.fraunhofer.de/jvet/HM/-/tree/HM-16.22">https://vcgit.hhi.fraunhofer.de/jvet/HM/-/tree/HM-16.22<i class="fas fa-external-link-alt"></i></a><br>[3] <a class="link" target="_blank" rel="noopener" href="https://vcgit.hhi.fraunhofer.de/jvet/VVCSoftware_VTM/-/tree/VTM-20.2">https://vcgit.hhi.fraunhofer.de/jvet/VVCSoftware_VTM/-/tree/VTM-20.2<i class="fas fa-external-link-alt"></i></a><br>[4] Common Test Conditions and Software Reference Configurations for HM (JCTVC-L1100)<br>[5] JVET common test conditions and software reference configurations (JVET-J1010)</p>

            
        </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            
<footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
                <span>2022</span> -
            
            2024
            
                &nbsp;<i class="fas fa-heart icon-animate"></i>
                &nbsp;<a href="/">Sponsored by ByteDance</a>
            
        </div>
        
            <script async 
                    src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                
                
                    Totalview&nbsp;<span id="busuanzi_value_site_pv"></span>
                
            </div>
        
        <div class="theme-info info-item">
            Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.5.2</a>
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    <div class="zoom-in-image-mask">
    <img class="zoom-in-image">
</div>


    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>







    
<script src="/js/code-block.js"></script>





<div class="post-scripts">
    
</div>



</body>
</html>
